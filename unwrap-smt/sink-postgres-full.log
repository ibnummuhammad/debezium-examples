	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2024-06-15 13:07:21,909 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 13:07:21,911 INFO   ||  These configurations '[group.id, rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.producer.ProducerConfig]
2024-06-15 13:07:21,911 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,911 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,911 INFO   ||  Kafka startTimeMs: 1718456841911   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,912 INFO   ||  ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 1-statuses
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 13:07:21,912 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 13:07:21,919 INFO   ||  These configurations '[rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 13:07:21,919 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,919 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,919 INFO   ||  Kafka startTimeMs: 1718456841919   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,926 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Cluster ID: lfAkuD2zQkStauD81hN01w   [org.apache.kafka.clients.Metadata]
2024-06-15 13:07:21,926 INFO   ||  [Producer clientId=1-statuses] Cluster ID: lfAkuD2zQkStauD81hN01w   [org.apache.kafka.clients.Metadata]
2024-06-15 13:07:21,929 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Assigned to partition(s): my_source_connect_statuses-0, my_source_connect_statuses-4, my_source_connect_statuses-1, my_source_connect_statuses-2, my_source_connect_statuses-3   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
2024-06-15 13:07:21,929 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,929 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-4   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,929 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-1   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,929 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-2   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,929 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-3   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,942 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,942 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,942 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,942 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,942 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:21,942 INFO   ||  Finished reading KafkaBasedLog for topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 13:07:21,942 INFO   ||  Started KafkaBasedLog for topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 13:07:21,946 INFO   ||  Starting KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2024-06-15 13:07:21,946 INFO   ||  Starting KafkaBasedLog with topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 13:07:21,980 INFO   ||  Created topic (name=my_connect_configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka:9092   [org.apache.kafka.connect.util.TopicAdmin]
2024-06-15 13:07:21,980 INFO   ||  ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = 1-configs
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2024-06-15 13:07:21,980 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 13:07:21,983 INFO   ||  These configurations '[group.id, rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.producer.ProducerConfig]
2024-06-15 13:07:21,983 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,983 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,983 INFO   ||  Kafka startTimeMs: 1718456841983   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,985 INFO   ||  ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 1-configs
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 13:07:21,985 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 13:07:21,988 INFO   ||  These configurations '[rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 13:07:21,988 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,988 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,988 INFO   ||  Kafka startTimeMs: 1718456841988   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:21,994 INFO   ||  [Producer clientId=1-configs] Cluster ID: lfAkuD2zQkStauD81hN01w   [org.apache.kafka.clients.Metadata]
2024-06-15 13:07:21,996 INFO   ||  [Consumer clientId=1-configs, groupId=1] Cluster ID: lfAkuD2zQkStauD81hN01w   [org.apache.kafka.clients.Metadata]
2024-06-15 13:07:21,997 INFO   ||  [Consumer clientId=1-configs, groupId=1] Assigned to partition(s): my_connect_configs-0   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
2024-06-15 13:07:21,998 INFO   ||  [Consumer clientId=1-configs, groupId=1] Seeking to earliest offset of partition my_connect_configs-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:22,007 INFO   ||  [Consumer clientId=1-configs, groupId=1] Resetting offset for partition my_connect_configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 13:07:22,007 INFO   ||  Finished reading KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 13:07:22,007 INFO   ||  Started KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 13:07:22,007 INFO   ||  Started KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2024-06-15 13:07:22,007 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Herder started   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:22,017 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Cluster ID: lfAkuD2zQkStauD81hN01w   [org.apache.kafka.clients.Metadata]
2024-06-15 13:07:22,821 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:22,824 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:22,824 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:22,839 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:25,882 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Successfully joined group with generation Generation{generationId=1, memberId='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:25,979 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Successfully synced group in generation Generation{generationId=1, memberId='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:25,980 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', leaderUrl='http://172.21.0.6:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:25,980 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Starting connectors and tasks using config offset -1   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:25,981 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:26,087 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Session key updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,008 INFO   ||  AbstractConfig values:
   [org.apache.kafka.common.config.AbstractConfig]
2024-06-15 13:07:56,042 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Connector jdbc-sink config updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,043 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:56,043 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:56,115 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Successfully joined group with generation Generation{generationId=2, memberId='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:56,127 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Successfully synced group in generation Generation{generationId=2, memberId='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:56,127 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', leaderUrl='http://172.21.0.6:8083/', offset=2, connectorIds=[jdbc-sink], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,128 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Starting connectors and tasks using config offset 2   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,134 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Starting connector jdbc-sink   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,147 INFO   ||  Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,148 INFO   ||  SinkConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [topic-kafka-postgres]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2024-06-15 13:07:56,154 INFO   ||  EnrichedConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [topic-kafka-postgres]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	transforms.timestampConverter.field = run_ts
	transforms.timestampConverter.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter.negate = false
	transforms.timestampConverter.predicate = null
	transforms.timestampConverter.target.type = Timestamp
	transforms.timestampConverter.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter.unix.precision = milliseconds
	transforms.timestampConverter1.field = etl_id_ts
	transforms.timestampConverter1.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter1.negate = false
	transforms.timestampConverter1.predicate = null
	transforms.timestampConverter1.target.type = Timestamp
	transforms.timestampConverter1.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter1.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field =
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2024-06-15 13:07:56,162 INFO   ||  Instantiated connector jdbc-sink with version 10.7.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,167 INFO   ||  Finished creating connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,168 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,170 INFO   ||  192.168.65.1 - - [15/Jun/2024:13:07:55 +0000] "POST /connectors/ HTTP/1.1" 201 1182 "-" "curl/8.6.0" 426   [org.apache.kafka.connect.runtime.rest.RestServer]
2024-06-15 13:07:56,191 INFO   ||  SinkConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [topic-kafka-postgres]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2024-06-15 13:07:56,195 INFO   ||  EnrichedConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [topic-kafka-postgres]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	transforms.timestampConverter.field = run_ts
	transforms.timestampConverter.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter.negate = false
	transforms.timestampConverter.predicate = null
	transforms.timestampConverter.target.type = Timestamp
	transforms.timestampConverter.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter.unix.precision = milliseconds
	transforms.timestampConverter1.field = etl_id_ts
	transforms.timestampConverter1.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter1.negate = false
	transforms.timestampConverter1.predicate = null
	transforms.timestampConverter1.target.type = Timestamp
	transforms.timestampConverter1.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter1.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field =
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2024-06-15 13:07:56,200 INFO   ||  Setting task configurations for 1 workers.   [io.confluent.connect.jdbc.JdbcSinkConnector]
2024-06-15 13:07:56,217 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Tasks [jdbc-sink-0] configs updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,219 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:56,219 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:56,221 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Successfully joined group with generation Generation{generationId=3, memberId='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:56,226 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Successfully synced group in generation Generation{generationId=3, memberId='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 13:07:56,226 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.21.0.6:8083-256fed9f-c41b-429a-90b1-508e3e6ac230', leaderUrl='http://172.21.0.6:8083/', offset=4, connectorIds=[jdbc-sink], taskIds=[jdbc-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,227 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Starting connectors and tasks using config offset 4   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,229 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Starting task jdbc-sink-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,235 INFO   ||  Creating task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,236 INFO   ||  ConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = [unwrap, timestampConverter, timestampConverter1]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2024-06-15 13:07:56,237 INFO   ||  EnrichedConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = [unwrap, timestampConverter, timestampConverter1]
	transforms.timestampConverter.field = run_ts
	transforms.timestampConverter.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter.negate = false
	transforms.timestampConverter.predicate = null
	transforms.timestampConverter.target.type = Timestamp
	transforms.timestampConverter.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter.unix.precision = milliseconds
	transforms.timestampConverter1.field = etl_id_ts
	transforms.timestampConverter1.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter1.negate = false
	transforms.timestampConverter1.predicate = null
	transforms.timestampConverter1.target.type = Timestamp
	transforms.timestampConverter1.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter1.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field =
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2024-06-15 13:07:56,239 INFO   ||  TaskConfig values:
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2024-06-15 13:07:56,239 INFO   ||  Instantiated task jdbc-sink-0 with version 10.7.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,240 INFO   ||  JsonConverterConfig values:
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2024-06-15 13:07:56,240 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,241 INFO   ||  JsonConverterConfig values:
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2024-06-15 13:07:56,241 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,241 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,251 WARN   ||  The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead.   [io.debezium.transforms.AbstractExtractNewRecordState]
2024-06-15 13:07:56,252 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState, org.apache.kafka.connect.transforms.TimestampConverter$Value, org.apache.kafka.connect.transforms.TimestampConverter$Value}   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 13:07:56,252 INFO   ||  SinkConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [topic-kafka-postgres]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2024-06-15 13:07:56,253 INFO   ||  EnrichedConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [topic-kafka-postgres]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	transforms.timestampConverter.field = run_ts
	transforms.timestampConverter.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter.negate = false
	transforms.timestampConverter.predicate = null
	transforms.timestampConverter.target.type = Timestamp
	transforms.timestampConverter.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter.unix.precision = milliseconds
	transforms.timestampConverter1.field = etl_id_ts
	transforms.timestampConverter1.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter1.negate = false
	transforms.timestampConverter1.predicate = null
	transforms.timestampConverter1.target.type = Timestamp
	transforms.timestampConverter1.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter1.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field =
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2024-06-15 13:07:56,254 INFO   ||  ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-jdbc-sink-0
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 13:07:56,255 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 13:07:56,264 INFO   ||  These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 13:07:56,264 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:56,264 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:56,264 INFO   ||  Kafka startTimeMs: 1718456876264   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 13:07:56,277 INFO   ||  [Worker clientId=connect-172.21.0.6:8083, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 13:07:56,277 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): topic-kafka-postgres   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
2024-06-15 13:07:56,279 INFO   ||  Starting JDBC Sink task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,279 INFO   ||  JdbcSinkConfig values:
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:postgresql://postgres:5432/data_warehouse
	connection.user = postgresuser
	date.timezone = DB_TIMEZONE
	db.timezone = UTC
	delete.enabled = false
	dialect.name =
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = data_warehouse.development_ibnu_muhammad.ekyc_pipeline_rt
	table.types = [TABLE]
	trim.sensitive.log = false
   [io.confluent.connect.jdbc.sink.JdbcSinkConfig]
2024-06-15 13:07:56,280 INFO   ||  masuk initWriter()...   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,280 INFO   ||  ...config: io.confluent.connect.jdbc.sink.JdbcSinkConfig@b30f2e3c   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,280 INFO   ||  ...config.dialectName:    [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,280 INFO   ||  ...config.dialectName.trim():    [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,280 INFO   ||  ...config.dialectName.trim().isEmpty(): true   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,280 INFO   ||  ...config.connectionUrl: jdbc:postgresql://postgres:5432/data_warehouse   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,280 INFO   ||  Initializing JDBC writer   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,280 INFO   ||  masuk findBestFor()...   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  ...findBestFor | jdbcUrl: jdbc:postgresql://postgres:5432/data_warehouse   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  ...findBestFor | config: io.confluent.connect.jdbc.sink.JdbcSinkConfig@b30f2e3c   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  masuk extractJdbcUrlInfo()...   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  ...extractJdbcUrlInfo | url: jdbc:postgresql://postgres:5432/data_warehouse   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  Validating JDBC URL.   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  ...extractJdbcUrlInfo | matcher: java.util.regex.Matcher[pattern=jdbc:([^:]+):(.*) region=0,46 lastmatch=]   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  ...extractJdbcUrlInfo | matcher.matches(): true   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  ...extractJdbcUrlInfo | matcher.group(1): postgresql   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  ...extractJdbcUrlInfo | matcher.group(2): //postgres:5432/data_warehouse   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,280 INFO   ||  Validated JDBC URL.   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,281 INFO   ||  Finding best dialect for JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,281 INFO   ||  ...findBestFor | bestScore: 0   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,281 INFO   ||  ...findBestFor | bestMatch...: null   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[db2, ibmdb, db2j]
a...subprotocol...
db2
a...subprotocol...
ibmdb
a...subprotocol...
db2j
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
db2
b...subprotocol...
ibmdb
b...subprotocol...
db2j
2024-06-15 13:07:56,282 INFO   ||  Dialect Db2DatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: null | bestScore 0   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[derby]
a...subprotocol...
derby
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
derby
2024-06-15 13:07:56,282 INFO   ||  Dialect DerbyDatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: null | bestScore 0   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  Dialect GenericDatabaseDialect scored 10 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: GenericDatabaseDialect | bestScore 10   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[mariadb, mysql]
a...subprotocol...
mariadb
a...subprotocol...
mysql
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
mariadb
b...subprotocol...
mysql
2024-06-15 13:07:56,282 INFO   ||  Dialect MySqlDatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: GenericDatabaseDialect | bestScore 10   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[oracle]
a...subprotocol...
oracle
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
oracle
2024-06-15 13:07:56,282 INFO   ||  Dialect OracleDatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: GenericDatabaseDialect | bestScore 10   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[postgresql]
a...subprotocol...
postgresql
2024-06-15 13:07:56,282 INFO   ||  Dialect PostgreSqlDatabaseDialect scored 100 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: PostgreSqlDatabaseDialect | bestScore 100   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[sap]
a...subprotocol...
sap
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
sap
2024-06-15 13:07:56,282 INFO   ||  Dialect SapHanaDatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: PostgreSqlDatabaseDialect | bestScore 100   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[sqlserver, microsoft:sqlserver, jtds:sqlserver]
a...subprotocol...
sqlserver
a...subprotocol...
microsoft:sqlserver
a...subprotocol...
jtds:sqlserver
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
sqlserver
b...subprotocol...
microsoft:sqlserver
b...subprotocol...
jtds:sqlserver
2024-06-15 13:07:56,282 INFO   ||  Dialect SqlServerDatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: PostgreSqlDatabaseDialect | bestScore 100   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[sqlite]
a...subprotocol...
sqlite
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
sqlite
2024-06-15 13:07:56,282 INFO   ||  Dialect SqliteDatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: PostgreSqlDatabaseDialect | bestScore 100   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[jtds:sybase, sqlserver, microsoft:sqlserver]
a...subprotocol...
jtds:sybase
a...subprotocol...
sqlserver
a...subprotocol...
microsoft:sqlserver
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
jtds:sybase
b...subprotocol...
sqlserver
b...subprotocol...
microsoft:sqlserver
2024-06-15 13:07:56,282 INFO   ||  Dialect SybaseDatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,282 INFO   ||  ...bestMatch: PostgreSqlDatabaseDialect | bestScore 100   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
...masuk score...
urlInfo...
JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'
urlInfo.subprotocol()...
postgresql
subprotocols...
[vertica]
a...subprotocol...
vertica
a...combined...
postgresql://postgres:5432/data_warehouse
b...combined...
postgresql://postgres:5432/data_warehouse
b...subprotocol...
vertica
2024-06-15 13:07:56,283 INFO   ||  Dialect VerticaDatabaseDialect scored 0 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,283 INFO   ||  ...bestMatch: PostgreSqlDatabaseDialect | bestScore 100   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,283 INFO   ||  Using dialect PostgreSqlDatabaseDialect with score 100 against JDBC subprotocol 'postgresql' and source 'jdbc:postgresql://postgres:5432/data_warehouse'   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,284 INFO   ||  masuk extractJdbcUrlInfo()...   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,284 INFO   ||  ...extractJdbcUrlInfo | url: jdbc:postgresql://postgres:5432/data_warehouse   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,284 INFO   ||  Validating JDBC URL.   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,284 INFO   ||  ...extractJdbcUrlInfo | matcher: java.util.regex.Matcher[pattern=jdbc:([^:]+):(.*) region=0,46 lastmatch=]   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,284 INFO   ||  ...extractJdbcUrlInfo | matcher.matches(): true   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,284 INFO   ||  ...extractJdbcUrlInfo | matcher.group(1): postgresql   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,284 INFO   ||  ...extractJdbcUrlInfo | matcher.group(2): //postgres:5432/data_warehouse   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,284 INFO   ||  Validated JDBC URL.   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 13:07:56,285 INFO   ||  ...dialect: PostgreSql   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,285 INFO   ||  ...dialect.getClass(): class io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,285 INFO   ||  ...dialect.getClass().getSimpleName(): PostgreSqlDatabaseDialect   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,286 INFO   ||  Initializing writer using SQL dialect: PostgreSqlDatabaseDialect   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,288 INFO   ||  JDBC writer initialized   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 13:07:56,289 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2024-06-15 13:07:56,292 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Executing sink task   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2024-06-15 13:07:56,335 WARN   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Error while fetching metadata with correlation id 2 : {topic-kafka-postgres=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2024-06-15 13:07:56,335 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: lfAkuD2zQkStauD81hN01w   [org.apache.kafka.clients.Metadata]
2024-06-15 13:07:56,336 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 13:07:56,337 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 13:07:56,350 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Request joining group due to: need to re-join with the given member-id: connector-consumer-jdbc-sink-0-38a246e2-31d9-4a43-981b-8adcd93c122b   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 13:07:56,350 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
