2024-06-15 08:33:54,749 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,749 INFO   ||  Kafka startTimeMs: 1718440434749   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,750 INFO   ||  ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 1-statuses
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 08:33:54,750 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 08:33:54,762 INFO   ||  These configurations '[rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 08:33:54,762 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,762 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,762 INFO   ||  Kafka startTimeMs: 1718440434762   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,765 INFO   ||  [Producer clientId=1-statuses] Cluster ID: fcIYrFVaRWuDkiC1XFuLag   [org.apache.kafka.clients.Metadata]
2024-06-15 08:33:54,767 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Cluster ID: fcIYrFVaRWuDkiC1XFuLag   [org.apache.kafka.clients.Metadata]
2024-06-15 08:33:54,768 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Assigned to partition(s): my_source_connect_statuses-0, my_source_connect_statuses-4, my_source_connect_statuses-1, my_source_connect_statuses-2, my_source_connect_statuses-3   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
2024-06-15 08:33:54,768 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,768 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-4   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,768 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-1   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,768 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-2   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,768 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Seeking to earliest offset of partition my_source_connect_statuses-3   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,776 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,777 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,777 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,777 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,777 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting offset for partition my_source_connect_statuses-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,780 INFO   ||  Finished reading KafkaBasedLog for topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:33:54,780 INFO   ||  Started KafkaBasedLog for topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:33:54,800 INFO   ||  Starting KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2024-06-15 08:33:54,801 INFO   ||  Starting KafkaBasedLog with topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:33:54,886 INFO   ||  Created topic (name=my_connect_configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka:9092   [org.apache.kafka.connect.util.TopicAdmin]
2024-06-15 08:33:54,887 INFO   ||  ProducerConfig values:
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [kafka:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = 1-configs
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
   [org.apache.kafka.clients.producer.ProducerConfig]
2024-06-15 08:33:54,887 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 08:33:54,891 INFO   ||  These configurations '[group.id, rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.producer.ProducerConfig]
2024-06-15 08:33:54,891 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,891 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,891 INFO   ||  Kafka startTimeMs: 1718440434891   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,892 INFO   ||  ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = 1-configs
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 1
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 08:33:54,893 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 08:33:54,897 INFO   ||  These configurations '[rest.advertised.port, task.shutdown.graceful.timeout.ms, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, rest.host.name, offset.flush.timeout.ms, config.storage.replication.factor, offset.flush.interval.ms, rest.port, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 08:33:54,898 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,898 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,898 INFO   ||  Kafka startTimeMs: 1718440434898   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:33:54,898 INFO   ||  [Producer clientId=1-configs] Cluster ID: fcIYrFVaRWuDkiC1XFuLag   [org.apache.kafka.clients.Metadata]
2024-06-15 08:33:54,902 INFO   ||  [Consumer clientId=1-configs, groupId=1] Cluster ID: fcIYrFVaRWuDkiC1XFuLag   [org.apache.kafka.clients.Metadata]
2024-06-15 08:33:54,903 INFO   ||  [Consumer clientId=1-configs, groupId=1] Assigned to partition(s): my_connect_configs-0   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
2024-06-15 08:33:54,903 INFO   ||  [Consumer clientId=1-configs, groupId=1] Seeking to earliest offset of partition my_connect_configs-0   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,909 INFO   ||  [Consumer clientId=1-configs, groupId=1] Resetting offset for partition my_connect_configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:33:54,910 INFO   ||  Finished reading KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:33:54,910 INFO   ||  Started KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:33:54,910 INFO   ||  Started KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2024-06-15 08:33:54,910 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Herder started   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:33:54,930 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Cluster ID: fcIYrFVaRWuDkiC1XFuLag   [org.apache.kafka.clients.Metadata]
2024-06-15 08:33:56,470 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:33:56,473 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:33:56,473 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:33:56,504 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:33:59,515 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Successfully joined group with generation Generation{generationId=1, memberId='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:33:59,565 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Successfully synced group in generation Generation{generationId=1, memberId='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:33:59,566 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', leaderUrl='http://192.168.96.6:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:33:59,567 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Starting connectors and tasks using config offset -1   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:33:59,567 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:33:59,642 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Session key updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,459 INFO   ||  AbstractConfig values:
   [org.apache.kafka.common.config.AbstractConfig]
2024-06-15 08:34:40,474 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Connector jdbc-sink config updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,475 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:34:40,475 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:34:40,482 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Successfully joined group with generation Generation{generationId=2, memberId='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:34:40,492 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Successfully synced group in generation Generation{generationId=2, memberId='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:34:40,492 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', leaderUrl='http://192.168.96.6:8083/', offset=2, connectorIds=[jdbc-sink], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,493 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Starting connectors and tasks using config offset 2   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,512 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Starting connector jdbc-sink   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,523 INFO   ||  Creating connector jdbc-sink of type io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,525 INFO   ||  SinkConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [kafka-postres-topic]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2024-06-15 08:34:40,528 INFO   ||  EnrichedConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [kafka-postres-topic]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	transforms.timestampConverter.field = run_ts
	transforms.timestampConverter.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter.negate = false
	transforms.timestampConverter.predicate = null
	transforms.timestampConverter.target.type = Timestamp
	transforms.timestampConverter.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter.unix.precision = milliseconds
	transforms.timestampConverter1.field = etl_id_ts
	transforms.timestampConverter1.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter1.negate = false
	transforms.timestampConverter1.predicate = null
	transforms.timestampConverter1.target.type = Timestamp
	transforms.timestampConverter1.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter1.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field =
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2024-06-15 08:34:40,543 INFO   ||  Instantiated connector jdbc-sink with version 10.7.6 of type class io.confluent.connect.jdbc.JdbcSinkConnector   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,545 INFO   ||  Finished creating connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,547 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,573 INFO   ||  192.168.65.1 - - [15/Jun/2024:08:34:40 +0000] "POST /connectors/ HTTP/1.1" 201 1181 "-" "curl/8.6.0" 300   [org.apache.kafka.connect.runtime.rest.RestServer]
2024-06-15 08:34:40,582 INFO   ||  SinkConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [kafka-postres-topic]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2024-06-15 08:34:40,585 INFO   ||  EnrichedConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [kafka-postres-topic]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	transforms.timestampConverter.field = run_ts
	transforms.timestampConverter.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter.negate = false
	transforms.timestampConverter.predicate = null
	transforms.timestampConverter.target.type = Timestamp
	transforms.timestampConverter.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter.unix.precision = milliseconds
	transforms.timestampConverter1.field = etl_id_ts
	transforms.timestampConverter1.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter1.negate = false
	transforms.timestampConverter1.predicate = null
	transforms.timestampConverter1.target.type = Timestamp
	transforms.timestampConverter1.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter1.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field =
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2024-06-15 08:34:40,588 INFO   ||  Setting task configurations for 1 workers.   [io.confluent.connect.jdbc.JdbcSinkConnector]
2024-06-15 08:34:40,631 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Tasks [jdbc-sink-0] configs updated   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,639 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Rebalance started   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:34:40,639 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] (Re-)joining group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:34:40,641 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Successfully joined group with generation Generation{generationId=3, memberId='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:34:40,649 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Successfully synced group in generation Generation{generationId=3, memberId='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', protocol='sessioned'}   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:34:40,649 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317', leaderUrl='http://192.168.96.6:8083/', offset=4, connectorIds=[jdbc-sink], taskIds=[jdbc-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,650 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Starting connectors and tasks using config offset 4   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,651 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Starting task jdbc-sink-0   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,654 INFO   ||  Creating task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,655 INFO   ||  ConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = [unwrap, timestampConverter, timestampConverter1]
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig]
2024-06-15 08:34:40,655 INFO   ||  EnrichedConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	transforms = [unwrap, timestampConverter, timestampConverter1]
	transforms.timestampConverter.field = run_ts
	transforms.timestampConverter.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter.negate = false
	transforms.timestampConverter.predicate = null
	transforms.timestampConverter.target.type = Timestamp
	transforms.timestampConverter.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter.unix.precision = milliseconds
	transforms.timestampConverter1.field = etl_id_ts
	transforms.timestampConverter1.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter1.negate = false
	transforms.timestampConverter1.predicate = null
	transforms.timestampConverter1.target.type = Timestamp
	transforms.timestampConverter1.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter1.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field =
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2024-06-15 08:34:40,657 INFO   ||  TaskConfig values:
	task.class = class io.confluent.connect.jdbc.sink.JdbcSinkTask
   [org.apache.kafka.connect.runtime.TaskConfig]
2024-06-15 08:34:40,657 INFO   ||  Instantiated task jdbc-sink-0 with version 10.7.6 of type io.confluent.connect.jdbc.sink.JdbcSinkTask   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,658 INFO   ||  JsonConverterConfig values:
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2024-06-15 08:34:40,658 INFO   ||  Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,658 INFO   ||  JsonConverterConfig values:
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
   [org.apache.kafka.connect.json.JsonConverterConfig]
2024-06-15 08:34:40,658 INFO   ||  Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,658 INFO   ||  Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task jdbc-sink-0 using the worker config   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,672 WARN   ||  The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead.   [io.debezium.transforms.AbstractExtractNewRecordState]
2024-06-15 08:34:40,675 INFO   ||  Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.transforms.ExtractNewRecordState, org.apache.kafka.connect.transforms.TimestampConverter$Value, org.apache.kafka.connect.transforms.TimestampConverter$Value}   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:34:40,675 INFO   ||  SinkConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [kafka-postres-topic]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	value.converter = null
   [org.apache.kafka.connect.runtime.SinkConnectorConfig]
2024-06-15 08:34:40,676 INFO   ||  EnrichedConnectorConfig values:
	config.action.reload = restart
	connector.class = io.confluent.connect.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name =
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = jdbc-sink
	predicates = []
	tasks.max = 1
	topics = [kafka-postres-topic]
	topics.regex =
	transforms = [unwrap, timestampConverter, timestampConverter1]
	transforms.timestampConverter.field = run_ts
	transforms.timestampConverter.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter.negate = false
	transforms.timestampConverter.predicate = null
	transforms.timestampConverter.target.type = Timestamp
	transforms.timestampConverter.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter.unix.precision = milliseconds
	transforms.timestampConverter1.field = etl_id_ts
	transforms.timestampConverter1.format = yyyy-MM-dd HH:mm:ss
	transforms.timestampConverter1.negate = false
	transforms.timestampConverter1.predicate = null
	transforms.timestampConverter1.target.type = Timestamp
	transforms.timestampConverter1.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestampConverter1.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field =
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = null
   [org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig]
2024-06-15 08:34:40,677 INFO   ||  ConsumerConfig values:
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-jdbc-sink-0
	client.rack =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-jdbc-sink
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 08:34:40,678 INFO   ||  initializing Kafka metrics collector   [org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector]
2024-06-15 08:34:40,689 INFO   ||  These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet.   [org.apache.kafka.clients.consumer.ConsumerConfig]
2024-06-15 08:34:40,692 INFO   ||  Kafka version: 3.7.0   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:34:40,692 INFO   ||  Kafka commitId: 2ae524ed625438c5   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:34:40,692 INFO   ||  Kafka startTimeMs: 1718440480689   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:34:40,707 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Finished starting connectors and tasks   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:34:40,708 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Subscribed to topic(s): kafka-postres-topic   [org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer]
2024-06-15 08:34:40,709 INFO   ||  Starting JDBC Sink task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 08:34:40,710 INFO   ||  JdbcSinkConfig values:
	auto.create = true
	auto.evolve = false
	batch.size = 3000
	connection.attempts = 3
	connection.backoff.ms = 10000
	connection.password = [hidden]
	connection.url = jdbc:postgresql://postgres:5432/data_warehouse
	connection.user = postgresuser
	date.timezone = DB_TIMEZONE
	db.timezone = UTC
	delete.enabled = false
	dialect.name =
	fields.whitelist = []
	insert.mode = insert
	max.retries = 10
	mssql.use.merge.holdlock = true
	pk.fields = []
	pk.mode = none
	quote.sql.identifiers = ALWAYS
	retry.backoff.ms = 3000
	table.name.format = data_warehouse.development_ibnu_muhammad.ekyc_pipeline_rt
	table.types = [TABLE]
	trim.sensitive.log = false
   [io.confluent.connect.jdbc.sink.JdbcSinkConfig]
2024-06-15 08:34:40,710 INFO   ||  Initializing JDBC writer   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 08:34:40,710 INFO   ||  Validating JDBC URL.   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 08:34:40,710 INFO   ||  Validated JDBC URL.   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 08:34:40,713 INFO   ||  Validating JDBC URL.   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 08:34:40,713 INFO   ||  Validated JDBC URL.   [io.confluent.connect.jdbc.dialect.DatabaseDialects]
2024-06-15 08:34:40,714 INFO   ||  Initializing writer using SQL dialect: PostgreSqlDatabaseDialect   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 08:34:40,715 INFO   ||  JDBC writer initialized   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 08:34:40,715 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Sink task finished initialization and start   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2024-06-15 08:34:40,716 INFO   ||  WorkerSinkTask{id=jdbc-sink-0} Executing sink task   [org.apache.kafka.connect.runtime.WorkerSinkTask]
2024-06-15 08:34:40,744 WARN   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Error while fetching metadata with correlation id 2 : {kafka-postres-topic=LEADER_NOT_AVAILABLE}   [org.apache.kafka.clients.NetworkClient]
2024-06-15 08:34:40,745 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Cluster ID: fcIYrFVaRWuDkiC1XFuLag   [org.apache.kafka.clients.Metadata]
2024-06-15 08:34:40,745 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:40,746 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:40,755 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Request joining group due to: need to re-join with the given member-id: connector-consumer-jdbc-sink-0-7c7ad1c0-d3cc-4e89-940b-7df97316fc95   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:40,756 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] (Re-)joining group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:43,769 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-jdbc-sink-0-7c7ad1c0-d3cc-4e89-940b-7df97316fc95', protocol='range'}   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:43,782 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Finished assignment for group at generation 1: {connector-consumer-jdbc-sink-0-7c7ad1c0-d3cc-4e89-940b-7df97316fc95=Assignment(partitions=[kafka-postres-topic-0])}   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:43,790 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-jdbc-sink-0-7c7ad1c0-d3cc-4e89-940b-7df97316fc95', protocol='range'}   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:43,790 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Notifying assignor about the new Assignment(partitions=[kafka-postres-topic-0])   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:43,791 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Adding newly assigned partitions: kafka-postres-topic-0   [org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker]
2024-06-15 08:34:43,806 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Found no committed offset for partition kafka-postres-topic-0   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:34:43,808 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting offset for partition kafka-postres-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1001 rack: null)], epoch=0}}.   [org.apache.kafka.clients.consumer.internals.SubscriptionState]
2024-06-15 08:35:07,145 INFO   ||  Maximum table name length for database is 63 bytes   [io.confluent.connect.jdbc.dialect.PostgreSqlDatabaseDialect]
2024-06-15 08:35:07,145 INFO   ||  JdbcDbWriter Connected   [io.confluent.connect.jdbc.sink.JdbcDbWriter]
2024-06-15 08:35:07,202 INFO   ||  mari...   [io.confluent.connect.jdbc.sink.BufferedRecords]
2024-06-15 08:35:07,203 INFO   ||  ...fieldsMetadata: FieldsMetadata{keyFieldNames=[], nonKeyFieldNames=[params, payload, etl_id, etl_id_ts, etl_id_partition, run_ts], allFields={params=SinkRecordField{schema=Schema{STRING}, name='params', isPrimaryKey=false}, payload=SinkRecordField{schema=Schema{STRING}, name='payload', isPrimaryKey=false}, etl_id=SinkRecordField{schema=Schema{STRING}, name='etl_id', isPrimaryKey=false}, etl_id_ts=SinkRecordField{schema=Schema{org.apache.kafka.connect.data.Timestamp:INT64}, name='etl_id_ts', isPrimaryKey=false}, etl_id_partition=SinkRecordField{schema=Schema{INT64}, name='etl_id_partition', isPrimaryKey=false}, run_ts=SinkRecordField{schema=Schema{org.apache.kafka.connect.data.Timestamp:INT64}, name='run_ts', isPrimaryKey=false}}}   [io.confluent.connect.jdbc.sink.BufferedRecords]
FieldsMetadata{keyFieldNames=[], nonKeyFieldNames=[params, payload, etl_id, etl_id_ts, etl_id_partition, run_ts], allFields={params=SinkRecordField{schema=Schema{STRING}, name='params', isPrimaryKey=false}, payload=SinkRecordField{schema=Schema{STRING}, name='payload', isPrimaryKey=false}, etl_id=SinkRecordField{schema=Schema{STRING}, name='etl_id', isPrimaryKey=false}, etl_id_ts=SinkRecordField{schema=Schema{org.apache.kafka.connect.data.Timestamp:INT64}, name='etl_id_ts', isPrimaryKey=false}, etl_id_partition=SinkRecordField{schema=Schema{INT64}, name='etl_id_partition', isPrimaryKey=false}, run_ts=SinkRecordField{schema=Schema{org.apache.kafka.connect.data.Timestamp:INT64}, name='run_ts', isPrimaryKey=false}}}
2024-06-15 08:35:07,211 INFO   ||  Checking PostgreSql dialect for existence of TABLE "data_warehouse"."development_ibnu_muhammad"."ekyc_pipeline_rt"   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,250 INFO   ||  Using PostgreSql dialect TABLE "data_warehouse"."development_ibnu_muhammad"."ekyc_pipeline_rt" absent   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,252 INFO   ||  testingg...   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,252 INFO   ||  ...f.schemaName(): null   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,252 INFO   ||  ...f.schemaType(): STRING   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,252 INFO   ||  ...f.name(): params   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  testingg...   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaName(): null   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaType(): STRING   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.name(): payload   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  testingg...   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaName(): null   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaType(): STRING   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.name(): etl_id   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  testingg...   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaName(): org.apache.kafka.connect.data.Timestamp   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaType(): INT64   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.name(): etl_id_ts   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  testingg...   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaName(): null   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaType(): INT64   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.name(): etl_id_partition   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  testingg...   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaName(): org.apache.kafka.connect.data.Timestamp   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.schemaType(): INT64   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  ...f.name(): run_ts   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,253 INFO   ||  Creating table with sql: CREATE TABLE "data_warehouse"."development_ibnu_muhammad"."ekyc_pipeline_rt" (
"params" TEXT NOT NULL,
"payload" TEXT NOT NULL,
"etl_id" TEXT NOT NULL,
"etl_id_ts" TIMESTAMP NOT NULL,
"etl_id_partition" BIGINT NOT NULL,
"run_ts" TIMESTAMP NOT NULL)   [io.confluent.connect.jdbc.sink.DbStructure]
2024-06-15 08:35:07,277 INFO   ||  Checking PostgreSql dialect for existence of TABLE "data_warehouse"."development_ibnu_muhammad"."ekyc_pipeline_rt"   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,278 INFO   ||  Using PostgreSql dialect TABLE "data_warehouse"."development_ibnu_muhammad"."ekyc_pipeline_rt" present   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,299 INFO   ||  Checking PostgreSql dialect for type of TABLE "data_warehouse"."development_ibnu_muhammad"."ekyc_pipeline_rt"   [io.confluent.connect.jdbc.dialect.GenericDatabaseDialect]
2024-06-15 08:35:07,301 INFO   ||  Setting metadata for table "data_warehouse"."development_ibnu_muhammad"."ekyc_pipeline_rt" to Table{name='"data_warehouse"."development_ibnu_muhammad"."ekyc_pipeline_rt"', type=TABLE columns=[Column{'payload', isPrimaryKey=false, allowsNull=false, sqlType=text}, Column{'etl_id_partition', isPrimaryKey=false, allowsNull=false, sqlType=int8}, Column{'params', isPrimaryKey=false, allowsNull=false, sqlType=text}, Column{'run_ts', isPrimaryKey=false, allowsNull=false, sqlType=timestamp}, Column{'etl_id_ts', isPrimaryKey=false, allowsNull=false, sqlType=timestamp}, Column{'etl_id', isPrimaryKey=false, allowsNull=false, sqlType=text}]}   [io.confluent.connect.jdbc.util.TableDefinitions]
2024-06-15 08:38:53,468 INFO   ||  [AdminClient clientId=1-shared-admin] Node -1 disconnected.   [org.apache.kafka.clients.NetworkClient]
2024-06-15 08:40:38,928 INFO   ||  Kafka Connect stopping   [org.apache.kafka.connect.runtime.Connect]
2024-06-15 08:40:38,930 INFO   ||  Stopping REST server   [org.apache.kafka.connect.runtime.rest.RestServer]
2024-06-15 08:40:38,946 INFO   ||  Stopped http_8083@75dbf41c{HTTP/1.1, (http/1.1)}{0.0.0.0:8083}   [org.eclipse.jetty.server.AbstractConnector]
2024-06-15 08:40:38,947 INFO   ||  node0 Stopped scavenging   [org.eclipse.jetty.server.session]
2024-06-15 08:40:38,962 INFO   ||  Stopped o.e.j.s.ServletContextHandler@5e2f409f{/,null,STOPPED}   [org.eclipse.jetty.server.handler.ContextHandler]
2024-06-15 08:40:38,963 INFO   ||  REST server stopped   [org.apache.kafka.connect.runtime.rest.RestServer]
2024-06-15 08:40:38,963 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Herder stopping   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:40:38,964 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Stopping connectors and tasks that are still assigned to this worker.   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:40:38,966 INFO   ||  Stopping connector jdbc-sink   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:40:38,966 INFO   ||  Scheduled shutdown for WorkerConnector{id=jdbc-sink}   [org.apache.kafka.connect.runtime.WorkerConnector]
2024-06-15 08:40:38,972 INFO   ||  Completed shutdown for WorkerConnector{id=jdbc-sink}   [org.apache.kafka.connect.runtime.WorkerConnector]
2024-06-15 08:40:38,973 INFO   ||  Stopping task jdbc-sink-0   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:40:38,974 INFO   ||  Stopping task   [io.confluent.connect.jdbc.sink.JdbcSinkTask]
2024-06-15 08:40:38,978 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Revoke previously assigned partitions kafka-postres-topic-0   [org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker]
2024-06-15 08:40:38,978 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Member connector-consumer-jdbc-sink-0-7c7ad1c0-d3cc-4e89-940b-7df97316fc95 sending LeaveGroup request to coordinator kafka:9092 (id: 2147482646 rack: null) due to the consumer is being closed   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:38,980 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:38,980 INFO   ||  [Consumer clientId=connector-consumer-jdbc-sink-0, groupId=connect-jdbc-sink] Request joining group due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:39,022 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,023 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,023 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,023 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,026 INFO   ||  App info kafka.consumer for connector-consumer-jdbc-sink-0 unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:39,029 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Member connect-192.168.96.6:8083-4908933e-7ecb-45f3-ada4-80ed58e0d317 sending LeaveGroup request to coordinator kafka:9092 (id: 2147482646 rack: null) due to the consumer is being closed   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:40:39,029 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:40:39,029 WARN   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Close timed out with 1 pending requests to coordinator, terminating client connections   [org.apache.kafka.connect.runtime.distributed.WorkerCoordinator]
2024-06-15 08:40:39,029 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,029 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,029 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,031 INFO   ||  App info kafka.connect for connect-192.168.96.6:8083 unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:39,031 INFO   ||  Stopping KafkaBasedLog for topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:40:39,032 INFO   ||  [Producer clientId=1-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2024-06-15 08:40:39,034 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,034 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,034 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,034 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,035 INFO   ||  App info kafka.producer for 1-statuses unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:39,035 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:39,035 INFO   ||  [Consumer clientId=1-statuses, groupId=1] Request joining group due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:39,534 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,534 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,534 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,534 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,537 INFO   ||  App info kafka.consumer for 1-statuses unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:39,537 INFO   ||  Stopped KafkaBasedLog for topic my_source_connect_statuses   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:40:39,537 INFO   ||  Closing KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2024-06-15 08:40:39,538 INFO   ||  Stopping KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:40:39,538 INFO   ||  [Producer clientId=1-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2024-06-15 08:40:39,545 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,545 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,545 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,545 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:39,545 INFO   ||  App info kafka.producer for 1-configs unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:39,546 INFO   ||  [Consumer clientId=1-configs, groupId=1] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:39,546 INFO   ||  [Consumer clientId=1-configs, groupId=1] Request joining group due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:40,029 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,029 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,030 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,030 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,032 INFO   ||  App info kafka.consumer for 1-configs unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:40,032 INFO   ||  Stopped KafkaBasedLog for topic my_connect_configs   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:40:40,032 INFO   ||  Closed KafkaConfigBackingStore   [org.apache.kafka.connect.storage.KafkaConfigBackingStore]
2024-06-15 08:40:40,032 INFO   ||  Worker stopping   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:40:40,034 INFO   ||  Stopping KafkaOffsetBackingStore   [org.apache.kafka.connect.storage.KafkaOffsetBackingStore]
2024-06-15 08:40:40,034 INFO   ||  Stopping KafkaBasedLog for topic my_connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:40:40,034 INFO   ||  [Producer clientId=1-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.   [org.apache.kafka.clients.producer.KafkaProducer]
2024-06-15 08:40:40,038 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,038 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,038 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,038 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,038 INFO   ||  App info kafka.producer for 1-offsets unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:40,039 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Resetting generation and member id due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:40,039 INFO   ||  [Consumer clientId=1-offsets, groupId=1] Request joining group due to: consumer pro-actively leaving the group   [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
2024-06-15 08:40:40,390 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,390 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,391 INFO   ||  Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,391 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,393 INFO   ||  App info kafka.consumer for 1-offsets unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:40,393 INFO   ||  Stopped KafkaBasedLog for topic my_connect_offsets   [org.apache.kafka.connect.util.KafkaBasedLog]
2024-06-15 08:40:40,393 INFO   ||  Stopped KafkaOffsetBackingStore   [org.apache.kafka.connect.storage.KafkaOffsetBackingStore]
2024-06-15 08:40:40,393 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,393 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,393 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,394 INFO   ||  App info kafka.connect for 192.168.96.6:8083 unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:40,394 INFO   ||  Worker stopped   [org.apache.kafka.connect.runtime.Worker]
2024-06-15 08:40:40,397 INFO   ||  App info kafka.admin.client for 1-shared-admin unregistered   [org.apache.kafka.common.utils.AppInfoParser]
2024-06-15 08:40:40,398 INFO   ||  Metrics scheduler closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,398 INFO   ||  Closing reporter org.apache.kafka.common.metrics.JmxReporter   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,398 INFO   ||  Metrics reporters closed   [org.apache.kafka.common.metrics.Metrics]
2024-06-15 08:40:40,398 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Herder stopped   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:40:40,400 INFO   ||  [Worker clientId=connect-192.168.96.6:8083, groupId=1] Herder stopped   [org.apache.kafka.connect.runtime.distributed.DistributedHerder]
2024-06-15 08:40:40,401 INFO   ||  Kafka Connect stopped   [org.apache.kafka.connect.runtime.Connect]
